# SentiSign

**Sign Language to Speech with Emotion**

SentiSign is a multi-modal system that translates sign language to speech while incorporating emotional context for natural, expressive communication.

## Project Vision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Webcam  â”‚â”€â”€â”€â–¶â”‚ Sign Languageâ”‚â”€â”€â”€â–¶â”‚  Context-   â”‚â”€â”€â”€â–¶â”‚Expressiveâ”‚â”€â”€â”€â–¶ ğŸ”Š
â”‚         â”‚    â”‚ Recognition  â”‚    â”‚  Aware      â”‚    â”‚   TTS    â”‚
â”‚         â”‚    â”‚ (Transformer)â”‚    â”‚Integration  â”‚    â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   (SLM)     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â–²
     â””â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Emotion    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚  Detection   â”‚
               â”‚  (ResNet)    â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Current Status

- âœ… **Emotion Detection** - Real-time facial emotion recognition (7 emotions)
- âœ… **Expressive TTS** - Emotion-aware speech synthesis using Parler TTS
- ğŸš§ **Sign Language Recognition** - In development (Transformer-based)
- ğŸ“‹ **Context Integration** - Planned (SLM-based)
- ğŸ“‹ **Full Pipeline** - Planned

## Quick Start

### Prerequisites

- Python 3.9 or higher
- Webcam
- (Optional) CUDA-capable GPU for faster inference

### Installation

1. Clone the repository:
```bash
git clone https://github.com/SHN2004/SentiSign.git
cd SentiSign
```

2. Create virtual environment with uv:
```bash
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install dependencies:
```bash
uv pip install -e .
```

### Running Demos

#### Emotion Detection Demo

```bash
python demos/run_emotion_webcam.py
```

Press 'q' to quit. The demo shows real-time emotion detection with bounding boxes and confidence scores.

#### Emotion + TTS Demo

```bash
python demos/run_emotion_tts.py --duration 5 --text "Hello, how are you doing?"
```

This demo:
1. Captures your emotion for 5 seconds
2. Determines the dominant emotion
3. Synthesizes speech with emotion-appropriate voice

#### Advanced Options

```bash
# Use specific camera
python demos/run_emotion_webcam.py --camera 1

# Disable mirror mode
python demos/run_emotion_webcam.py --no-flip

# Use CLAHE preprocessing for better contrast
python demos/run_emotion_webcam.py --clahe

# Longer capture for TTS
python demos/run_emotion_tts.py --duration 10

# Custom TTS output
python demos/run_emotion_tts.py --text "Your custom message" --output my_audio.wav
```

## Project Structure

```
SentiSign/
â”œâ”€â”€ sentisign/              # Main source code package
â”‚   â”œâ”€â”€ emotion/            # Emotion detection module
â”‚   â”œâ”€â”€ sign_language/      # Sign language recognition (in progress)
â”‚   â”œâ”€â”€ tts/                # Text-to-speech module
â”‚   â”œâ”€â”€ integration/        # Context-aware integration (planned)
â”‚   â””â”€â”€ utils/              # Shared utilities
â”œâ”€â”€ models/                 # Pre-trained model weights
â”‚   â””â”€â”€ emotion/            # Emotion detection models
â”œâ”€â”€ demos/                  # Standalone demonstration scripts
â”œâ”€â”€ docs/                   # Documentation
â””â”€â”€ tests/                  # Unit tests
```

## Modules

### Emotion Detection

7-class facial emotion recognition using ResNet-inspired CNN:
- Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise
- Real-time webcam processing
- 44Ã—44 input resolution
- See [sentisign/emotion/README.md](sentisign/emotion/README.md)

### Expressive TTS

Emotion-aware speech synthesis using Parler TTS:
- Voice characteristics adapt to detected emotion
- 7 preset emotion voice profiles
- Custom voice description support
- See [sentisign/tts/README.md](sentisign/tts/README.md)

### Sign Language Recognition (Coming Soon)

Transformer-based sign language gesture recognition.
- See [sentisign/sign_language/README.md](sentisign/sign_language/README.md)

### Context Integration (Planned)

SLM-based integration of sign language and emotion.
- See [sentisign/integration/README.md](sentisign/integration/README.md)

## Documentation

- [Architecture](docs/ARCHITECTURE.md) - System architecture and data flow
- [Setup Guide](docs/SETUP.md) - Detailed installation and environment setup
- [Contributing](docs/CONTRIBUTING.md) - Contribution guidelines and workflow
- [Module Guide](docs/MODULE_GUIDE.md) - How to add new modules

## Technology Stack

- **Deep Learning**: PyTorch 2.0+
- **Computer Vision**: OpenCV
- **TTS**: Parler TTS (Hugging Face Transformers)
- **Package Management**: uv
- **Language**: Python 3.9+

## Hardware Support

- **CUDA** (NVIDIA GPUs) - Full support, recommended for production
- **MPS** (Apple Silicon) - Supported for emotion detection
- **CPU** - Supported but slower

## Team

Built with collaboration and passion by the SentiSign team.

## License

[Add license information]

## Acknowledgments

- Parler TTS model from Hugging Face
- OpenCV for computer vision utilities
- PyTorch for deep learning framework

## Contact

For questions or suggestions, please open an issue on GitHub.

---

**Note**: This project is under active development. Features marked as "Coming Soon" or "Planned" are being worked on.
